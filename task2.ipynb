{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c71052f-9d6b-49c6-ad99-f89dd88d2a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Matrix for Saved Data 1:\n",
      " [[0.8488     0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.8488     0.85000002 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.8488     0.85039997 0.84600002 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.8484     0.85039997 0.84600002 0.85039997 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.8488     0.85039997 0.84600002 0.85039997 0.84960002 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.8488     0.85039997 0.84600002 0.85039997 0.84960002 0.84320003\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.8488     0.85079998 0.84600002 0.85000002 0.84920001 0.84320003\n",
      "  0.83560002 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.8488     0.85000002 0.84640002 0.85000002 0.84920001 0.84320003\n",
      "  0.83560002 0.84560001 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.8488     0.85039997 0.84640002 0.85000002 0.84920001 0.84320003\n",
      "  0.83520001 0.84560001 0.83920002 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.8488     0.85039997 0.84640002 0.85000002 0.84920001 0.84320003\n",
      "  0.83520001 0.84560001 0.83920002 0.8624     0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.8484     0.85039997 0.84640002 0.85039997 0.84920001 0.84320003\n",
      "  0.83560002 0.84560001 0.83920002 0.86199999 0.70279998 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.8484     0.85079998 0.84640002 0.85000002 0.84920001 0.84320003\n",
      "  0.83560002 0.84560001 0.83840001 0.86199999 0.70359999 0.54159999\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.8484     0.85039997 0.84640002 0.85000002 0.84960002 0.84320003\n",
      "  0.83560002 0.84560001 0.83840001 0.86199999 0.70319998 0.54159999\n",
      "  0.75520003 0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.8484     0.85039997 0.84600002 0.85000002 0.84960002 0.84320003\n",
      "  0.8348     0.84560001 0.83840001 0.86159998 0.70319998 0.54159999\n",
      "  0.75520003 0.7992     0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.8484     0.85039997 0.84600002 0.85000002 0.84960002 0.84320003\n",
      "  0.8348     0.84560001 0.83840001 0.86159998 0.70319998 0.54159999\n",
      "  0.75520003 0.79960001 0.83359998 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.84759998 0.85039997 0.84600002 0.85000002 0.84960002 0.84280002\n",
      "  0.8348     0.84560001 0.83840001 0.86159998 0.70319998 0.54159999\n",
      "  0.75520003 0.79960001 0.83359998 0.71719998 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.84759998 0.85039997 0.84640002 0.85000002 0.84960002 0.84320003\n",
      "  0.8344     0.84560001 0.83840001 0.86159998 0.70319998 0.542\n",
      "  0.75520003 0.79960001 0.83359998 0.71719998 0.72320002 0.\n",
      "  0.         0.        ]\n",
      " [0.84759998 0.85039997 0.84640002 0.85000002 0.84960002 0.84359998\n",
      "  0.83399999 0.84560001 0.83840001 0.86159998 0.70319998 0.542\n",
      "  0.75480002 0.79960001 0.83359998 0.71719998 0.72240001 0.73680001\n",
      "  0.         0.        ]\n",
      " [0.84759998 0.85039997 0.84640002 0.85000002 0.84960002 0.84320003\n",
      "  0.8344     0.84560001 0.83840001 0.86199999 0.70319998 0.54119998\n",
      "  0.75480002 0.79960001 0.83359998 0.71759999 0.72280002 0.73680001\n",
      "  0.62400001 0.        ]\n",
      " [0.84759998 0.85039997 0.84640002 0.85000002 0.85000002 0.84280002\n",
      "  0.8344     0.84560001 0.83840001 0.8624     0.70279998 0.54119998\n",
      "  0.75480002 0.79960001 0.83359998 0.71719998 0.72280002 0.73680001\n",
      "  0.62360001 0.8136    ]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_matrix_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 153\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# accuracy_matrix_2 = continual_learning_pipeline(data_path_2, eval_data_path_2)\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy Matrix for Saved Data 1:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, accuracy_matrix_1)\n\u001b[1;32m--> 153\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy Matrix for Saved Data 2:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[43maccuracy_matrix_2\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy_matrix_2' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load dataset features and labels\n",
    "def load_dataset(path, labeled=True):\n",
    "    data = torch.load(path)\n",
    "    if labeled:\n",
    "        return data['features'], data['labels']\n",
    "    return data['features']\n",
    "\n",
    "# Calculate class prototypes\n",
    "def calculate_prototypes(features, labels, num_classes=10):\n",
    "    prototypes = []\n",
    "    for c in range(num_classes):\n",
    "        # Ensure we're working with the correct feature dimension\n",
    "        class_features = features[labels == c]\n",
    "        if len(class_features) > 0:\n",
    "            prototypes.append(class_features.mean(dim=0))\n",
    "        else:\n",
    "            # If no features for a class, create a zero vector of correct dimension\n",
    "            prototypes.append(torch.zeros(features.shape[1]))\n",
    "    return torch.stack(prototypes)\n",
    "\n",
    "# Prototype-based prediction\n",
    "def predict(features, prototypes):\n",
    "    # Ensure features are float type\n",
    "    features = features.float()\n",
    "    prototypes = prototypes.float()\n",
    "    \n",
    "    # Use cosine similarity instead of Euclidean distance for high-dimensional features\n",
    "    similarities = F.cosine_similarity(features.unsqueeze(1), prototypes.unsqueeze(0), dim=2)\n",
    "    return torch.argmax(similarities, dim=1)\n",
    "\n",
    "# Prototype contrastive learning loss\n",
    "def prototype_contrastive_loss(features, pseudo_labels, prototypes, temperature=0.1):\n",
    "    # Ensure features and prototypes are float\n",
    "    features = features.float()\n",
    "    prototypes = prototypes.float()\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    logits = F.cosine_similarity(features.unsqueeze(1), prototypes.unsqueeze(0), dim=2) / temperature\n",
    "    loss = F.cross_entropy(logits, pseudo_labels)\n",
    "    return loss\n",
    "\n",
    "# Knowledge distillation loss\n",
    "def knowledge_distillation_loss(student_logits, teacher_logits, temperature=2.0):\n",
    "    teacher_probs = F.softmax(teacher_logits / temperature, dim=1)\n",
    "    student_probs = F.log_softmax(student_logits / temperature, dim=1)\n",
    "    return F.kl_div(student_probs, teacher_probs, reduction=\"batchmean\") * (temperature ** 2)\n",
    "\n",
    "# Training on D1\n",
    "def train_labeled_dataset(features, labels, num_classes=10):\n",
    "    prototypes = calculate_prototypes(features, labels, num_classes)\n",
    "    return prototypes\n",
    "\n",
    "# Update prototypes using PCL and KD\n",
    "def continual_learning(features, prototypes, prev_model, pseudo_labels, num_classes=10, lr=0.01):\n",
    "    # Ensure all tensors are float\n",
    "    features = features.float()\n",
    "    prototypes = prototypes.float()\n",
    "    \n",
    "    # Make prototypes a learnable parameter\n",
    "    prototypes = torch.nn.Parameter(prototypes.clone(), requires_grad=True)\n",
    "    optimizer = torch.optim.SGD([prototypes], lr=lr)\n",
    "    \n",
    "    for _ in range(50):  # Increased iterations for better convergence\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Prototype Contrastive Learning Loss\n",
    "        pcl_loss = prototype_contrastive_loss(features, pseudo_labels, prototypes)\n",
    "        \n",
    "        # Knowledge Distillation Loss (if previous model exists)\n",
    "        if prev_model is not None:\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = F.cosine_similarity(features.unsqueeze(1), prev_model.unsqueeze(0), dim=2)\n",
    "            student_logits = F.cosine_similarity(features.unsqueeze(1), prototypes.unsqueeze(0), dim=2)\n",
    "            kd_loss = knowledge_distillation_loss(student_logits, teacher_logits)\n",
    "            loss = pcl_loss + kd_loss\n",
    "        else:\n",
    "            loss = pcl_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return prototypes.detach()\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate(features, labels, prototypes):\n",
    "    predictions = predict(features, prototypes)\n",
    "    accuracy = (predictions == labels).float().mean().item()\n",
    "    return accuracy\n",
    "\n",
    "# Main continual learning pipeline\n",
    "def continual_learning_pipeline(data_path, eval_data_path, num_datasets=20, num_classes=10):\n",
    "    accuracy_matrix = np.zeros((num_datasets, num_datasets))\n",
    "    prev_prototypes = None\n",
    "    # Load labeled dataset (D1)\n",
    "    train_features, train_labels = load_dataset(f\"{data_path}/features_dataset_1.pth\")\n",
    "    \n",
    "    # Ensure features are float and have the expected 1280-dimensional shape\n",
    "    train_features = train_features.float()\n",
    "    assert train_features.shape[1] == 1280, f\"Expected 1280 features, got {train_features.shape[1]}\"\n",
    "    \n",
    "    prototypes = train_labeled_dataset(train_features, train_labels, num_classes)\n",
    "    \n",
    "    for i in range(1, num_datasets + 1):\n",
    "        if i > 1 and i<11:  # Unlabeled datasets\n",
    "            train_features = load_dataset(f\"{data_path}/features_dataset_{i}.pth\", labeled=False)\n",
    "            \n",
    "            # Ensure features are float and have the expected 1280-dimensional shape\n",
    "            train_features = train_features.float()\n",
    "            assert train_features.shape[1] == 1280, f\"Expected 1280 features, got {train_features.shape[1]}\"\n",
    "            \n",
    "            pseudo_labels = predict(train_features, prototypes)\n",
    "            prototypes = continual_learning(train_features, prototypes, prev_prototypes, pseudo_labels, num_classes)\n",
    "        elif i>=11:\n",
    "            train_features = load_dataset(f\"{data_path+'2'}/features_dataset_{i-10}.pth\", labeled=False)\n",
    "            \n",
    "            # Ensure features are float and have the expected 1280-dimensional shape\n",
    "            train_features = train_features.float()\n",
    "            assert train_features.shape[1] == 1280, f\"Expected 1280 features, got {train_features.shape[1]}\"\n",
    "            \n",
    "            pseudo_labels = predict(train_features, prototypes)\n",
    "            prototypes = continual_learning(train_features, prototypes, prev_prototypes, pseudo_labels, num_classes)\n",
    "        prev_prototypes = prototypes.clone()\n",
    "        \n",
    "        # Evaluate on all evaluation datasets from 1 to i\n",
    "        for j in range(1, i + 1):\n",
    "            if j<11:\n",
    "                eval_features, eval_labels = load_dataset(f\"{eval_data_path}/eval_features_dataset_{j}.pth\")\n",
    "            else:\n",
    "                eval_features, eval_labels = load_dataset(f\"{eval_data_path+'2'}/eval_features_dataset_{j-10}.pth\")\n",
    "            # Ensure features are float and have the expected 1280-dimensional shape\n",
    "            eval_features = eval_features.float()\n",
    "            assert eval_features.shape[1] == 1280, f\"Expected 1280 features, got {eval_features.shape[1]}\"\n",
    "            \n",
    "            accuracy_matrix[i - 1, j - 1] = evaluate(eval_features, eval_labels, prototypes)\n",
    "    \n",
    "    return accuracy_matrix\n",
    "\n",
    "# Run pipeline for both datasets (saved_data and saved_data2)\n",
    "data_path_1 = \"saved_data\"\n",
    "eval_data_path_1 = \"saved_data\"\n",
    "accuracy_matrix_1 = continual_learning_pipeline(data_path_1, eval_data_path_1)\n",
    "\n",
    "data_path_2 = \"saved_data2\"\n",
    "eval_data_path_2 = \"saved_data2\"\n",
    "# accuracy_matrix_2 = continual_learning_pipeline(data_path_2, eval_data_path_2)\n",
    "\n",
    "print(\"Accuracy Matrix for Saved Data 1:\\n\", accuracy_matrix_1)\n",
    "print(\"Accuracy Matrix for Saved Data 2:\\n\", accuracy_matrix_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5ba8ba-bf52-449a-b5ec-6e6b68cb22fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
