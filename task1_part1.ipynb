{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training initial model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chait\\AppData\\Local\\Temp\\ipykernel_37780\\2106438498.py:155: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  initial_data = torch.load('part_one_dataset/train_data/1_train_data.tar.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chait\\AppData\\Local\\Temp\\ipykernel_37780\\2106438498.py:160: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_data = torch.load(f'part_one_dataset/train_data/{i}_train_data.tar.pth')\n",
      "C:\\Users\\chait\\AppData\\Local\\Temp\\ipykernel_37780\\2106438498.py:164: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eval_data = torch.load(f'part_one_dataset/eval_data/{j}_eval_data.tar.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2, Dataset 1: Accuracy = 58.20%\n",
      "Model 2, Dataset 2: Accuracy = 58.80%\n",
      "Processing dataset 3...\n",
      "Model 3, Dataset 1: Accuracy = 57.00%\n",
      "Model 3, Dataset 2: Accuracy = 58.56%\n",
      "Model 3, Dataset 3: Accuracy = 57.28%\n",
      "Processing dataset 4...\n",
      "Model 4, Dataset 1: Accuracy = 56.32%\n",
      "Model 4, Dataset 2: Accuracy = 58.12%\n",
      "Model 4, Dataset 3: Accuracy = 56.80%\n",
      "Model 4, Dataset 4: Accuracy = 56.28%\n",
      "Processing dataset 5...\n",
      "Model 5, Dataset 1: Accuracy = 55.76%\n",
      "Model 5, Dataset 2: Accuracy = 57.56%\n",
      "Model 5, Dataset 3: Accuracy = 56.20%\n",
      "Model 5, Dataset 4: Accuracy = 55.76%\n",
      "Model 5, Dataset 5: Accuracy = 56.20%\n",
      "Processing dataset 6...\n",
      "Model 6, Dataset 1: Accuracy = 55.36%\n",
      "Model 6, Dataset 2: Accuracy = 56.92%\n",
      "Model 6, Dataset 3: Accuracy = 55.60%\n",
      "Model 6, Dataset 4: Accuracy = 55.24%\n",
      "Model 6, Dataset 5: Accuracy = 56.24%\n",
      "Model 6, Dataset 6: Accuracy = 55.80%\n",
      "Processing dataset 7...\n",
      "Model 7, Dataset 1: Accuracy = 55.16%\n",
      "Model 7, Dataset 2: Accuracy = 56.52%\n",
      "Model 7, Dataset 3: Accuracy = 55.08%\n",
      "Model 7, Dataset 4: Accuracy = 55.16%\n",
      "Model 7, Dataset 5: Accuracy = 56.00%\n",
      "Model 7, Dataset 6: Accuracy = 55.32%\n",
      "Model 7, Dataset 7: Accuracy = 55.36%\n",
      "Processing dataset 8...\n",
      "Model 8, Dataset 1: Accuracy = 54.76%\n",
      "Model 8, Dataset 2: Accuracy = 56.60%\n",
      "Model 8, Dataset 3: Accuracy = 54.56%\n",
      "Model 8, Dataset 4: Accuracy = 54.88%\n",
      "Model 8, Dataset 5: Accuracy = 55.68%\n",
      "Model 8, Dataset 6: Accuracy = 55.36%\n",
      "Model 8, Dataset 7: Accuracy = 55.08%\n",
      "Model 8, Dataset 8: Accuracy = 54.52%\n",
      "Processing dataset 9...\n",
      "Model 9, Dataset 1: Accuracy = 54.44%\n",
      "Model 9, Dataset 2: Accuracy = 56.12%\n",
      "Model 9, Dataset 3: Accuracy = 54.40%\n",
      "Model 9, Dataset 4: Accuracy = 54.52%\n",
      "Model 9, Dataset 5: Accuracy = 55.76%\n",
      "Model 9, Dataset 6: Accuracy = 55.24%\n",
      "Model 9, Dataset 7: Accuracy = 55.24%\n",
      "Model 9, Dataset 8: Accuracy = 54.44%\n",
      "Model 9, Dataset 9: Accuracy = 53.88%\n",
      "Processing dataset 10...\n",
      "Model 10, Dataset 1: Accuracy = 54.52%\n",
      "Model 10, Dataset 2: Accuracy = 55.36%\n",
      "Model 10, Dataset 3: Accuracy = 54.12%\n",
      "Model 10, Dataset 4: Accuracy = 54.20%\n",
      "Model 10, Dataset 5: Accuracy = 55.36%\n",
      "Model 10, Dataset 6: Accuracy = 54.24%\n",
      "Model 10, Dataset 7: Accuracy = 54.64%\n",
      "Model 10, Dataset 8: Accuracy = 54.08%\n",
      "Model 10, Dataset 9: Accuracy = 53.20%\n",
      "Model 10, Dataset 10: Accuracy = 56.04%\n",
      "\n",
      "Accuracy Matrix (%):\n",
      "Model ID | Dataset  1 Dataset  2 Dataset  3 Dataset  4 Dataset  5 Dataset  6 Dataset  7 Dataset  8 Dataset  9 Dataset 10\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "Model  1 |     0.00                                                                                 \n",
      "Model  2 |    58.20    58.80                                                                        \n",
      "Model  3 |    57.00    58.56    57.28                                                               \n",
      "Model  4 |    56.32    58.12    56.80    56.28                                                      \n",
      "Model  5 |    55.76    57.56    56.20    55.76    56.20                                             \n",
      "Model  6 |    55.36    56.92    55.60    55.24    56.24    55.80                                    \n",
      "Model  7 |    55.16    56.52    55.08    55.16    56.00    55.32    55.36                           \n",
      "Model  8 |    54.76    56.60    54.56    54.88    55.68    55.36    55.08    54.52                  \n",
      "Model  9 |    54.44    56.12    54.40    54.52    55.76    55.24    55.24    54.44    53.88         \n",
      "Model 10 |    54.52    55.36    54.12    54.20    55.36    54.24    54.64    54.08    53.20    56.04\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.models as models\n",
    "from collections import defaultdict\n",
    "\n",
    "# Feature extractor using pretrained ResNet18\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])  # Remove FC layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten output\n",
    "        return x\n",
    "\n",
    "# Linear classifier with softmax prediction\n",
    "class LwPClassifier(nn.Module):\n",
    "    def __init__(self, feature_dim=512, num_classes=10):\n",
    "        super(LwPClassifier, self).__init__()\n",
    "        self.classifier = nn.Linear(feature_dim, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        with torch.no_grad():\n",
    "            return self.softmax(self.forward(x))\n",
    "\n",
    "# Sequential learning pipeline\n",
    "class SequentialLearner:\n",
    "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.device = device\n",
    "        self.feature_extractor = FeatureExtractor().to(self.device)\n",
    "        self.feature_extractor.eval()  # Freeze feature extractor\n",
    "        self.models = {}\n",
    "        self.results = defaultdict(dict)\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "        # Normalize and preprocess for ImageNet format\n",
    "        data = data.astype(np.float32) / 255.0\n",
    "        data = (data - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "        data = data.transpose(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
    "        return torch.FloatTensor(data)\n",
    "\n",
    "    def extract_features(self, data):\n",
    "        data = self.preprocess_data(data)\n",
    "        data = data.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            features = self.feature_extractor(data)\n",
    "        return features\n",
    "\n",
    "    def train_initial_model(self, data, targets, model_id=1):\n",
    "        features = self.extract_features(data)\n",
    "        targets = torch.LongTensor(targets)\n",
    "\n",
    "        # Initialize model\n",
    "        model = LwPClassifier().to(self.device)\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Dataset and DataLoader\n",
    "        dataset = TensorDataset(features, targets)\n",
    "        dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for epoch in range(10):  # Adjust epochs if needed\n",
    "            for batch_features, batch_targets in dataloader:\n",
    "                batch_features, batch_targets = batch_features.to(self.device), batch_targets.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_features)\n",
    "                loss = criterion(outputs, batch_targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        self.models[model_id] = model\n",
    "        return model\n",
    "\n",
    "    def update_model(self, data, prev_model_id, new_model_id):\n",
    "        features = self.extract_features(data)\n",
    "\n",
    "        # Get predictions from previous model\n",
    "        prev_model = self.models[prev_model_id]\n",
    "        prev_model.eval()\n",
    "        with torch.no_grad():\n",
    "            pseudo_labels_prob = prev_model.predict_proba(features)\n",
    "            pseudo_labels = torch.argmax(pseudo_labels_prob, dim=1)\n",
    "\n",
    "        # Initialize new model and load weights from the previous model\n",
    "        new_model = LwPClassifier().to(self.device)\n",
    "        new_model.load_state_dict(prev_model.state_dict())\n",
    "\n",
    "        optimizer = optim.Adam(new_model.parameters())\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Dataset and DataLoader\n",
    "        dataset = TensorDataset(features, pseudo_labels)\n",
    "        dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "        # Knowledge distillation\n",
    "        temperature = 2.0\n",
    "        alpha = 0.5\n",
    "\n",
    "        new_model.train()\n",
    "        for epoch in range(5):  # Fewer epochs for updates\n",
    "            for batch_features, batch_pseudo_labels in dataloader:\n",
    "                batch_features = batch_features.to(self.device)\n",
    "                batch_pseudo_labels = batch_pseudo_labels.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Soft targets from previous model\n",
    "                with torch.no_grad():\n",
    "                    soft_targets = prev_model.predict_proba(batch_features)\n",
    "\n",
    "                # Current model predictions\n",
    "                outputs = new_model(batch_features)\n",
    "                outputs_soft = new_model.softmax(outputs / temperature)\n",
    "\n",
    "                # Combine hard and soft losses\n",
    "                loss_hard = criterion(outputs, batch_pseudo_labels)\n",
    "                loss_soft = nn.KLDivLoss(reduction='batchmean')(\n",
    "                    torch.log(outputs_soft),\n",
    "                    soft_targets\n",
    "                ) * (temperature ** 2)\n",
    "\n",
    "                loss = alpha * loss_hard + (1 - alpha) * loss_soft\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        self.models[new_model_id] = new_model\n",
    "        return new_model\n",
    "\n",
    "    def evaluate_model(self, model_id, eval_data, eval_targets):\n",
    "        model = self.models[model_id]\n",
    "        features = self.extract_features(eval_data)\n",
    "        targets = torch.LongTensor(eval_targets).to(self.device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(features.to(self.device))\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (predictions == targets).float().mean().item()\n",
    "\n",
    "        return accuracy * 100\n",
    "\n",
    "    def run_sequential_learning(self, num_models=10):\n",
    "        print(\"Training initial model...\")\n",
    "        initial_data = torch.load('part_one_dataset/train_data/1_train_data.tar.pth')\n",
    "        self.train_initial_model(initial_data['data'], initial_data['targets'])\n",
    "\n",
    "        for i in range(2, num_models + 1):\n",
    "            print(f\"Processing dataset {i}...\")\n",
    "            train_data = torch.load(f'part_one_dataset/train_data/{i}_train_data.tar.pth')\n",
    "            self.update_model(train_data['data'], i - 1, i)\n",
    "\n",
    "            for j in range(1, i + 1):\n",
    "                eval_data = torch.load(f'part_one_dataset/eval_data/{j}_eval_data.tar.pth')\n",
    "                accuracy = self.evaluate_model(i, eval_data['data'], eval_data['targets'])\n",
    "                self.results[i][j] = accuracy\n",
    "                print(f\"Model {i}, Dataset {j}: Accuracy = {accuracy:.2f}%\")\n",
    "\n",
    "        return self.results\n",
    "\n",
    "def print_results_matrix(results, num_models=10):\n",
    "    print(\"\\nAccuracy Matrix (%):\")\n",
    "    print(\"Model ID | \" + \" \".join(f\"Dataset {i:2d}\" for i in range(1, num_models + 1)))\n",
    "    print(\"-\" * (9 + num_models * 11))\n",
    "\n",
    "    for model_id in range(1, num_models + 1):\n",
    "        row = [f\"Model {model_id:2d} |\"]\n",
    "        for dataset_id in range(1, num_models + 1):\n",
    "            accuracy = results.get(model_id, {}).get(dataset_id, 0.0)\n",
    "            if dataset_id <= model_id:\n",
    "                row.append(f\"{accuracy:8.2f}\")\n",
    "            else:\n",
    "                row.append(\" \" * 8)\n",
    "        print(\" \".join(row))\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    learner = SequentialLearner()\n",
    "    results = learner.run_sequential_learning()\n",
    "    print_results_matrix(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
