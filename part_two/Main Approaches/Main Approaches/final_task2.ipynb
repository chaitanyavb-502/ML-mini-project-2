{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on labeled data from saved_data...\n",
      "Continual learning on saved_data2 (unlabeled)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2796\\3885488547.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(path)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2796\\3885488547.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  features = torch.tensor(features, dtype=torch.float32)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2796\\3885488547.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Matrix for Saved Data 2:\n",
      " [[0.69440001 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.69400001 0.53839999 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.69400001 0.53839999 0.74720001 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.69400001 0.53839999 0.74720001 0.79159999 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.69400001 0.53839999 0.74720001 0.79159999 0.83039999 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.69400001 0.53839999 0.74720001 0.79159999 0.82999998 0.71359998\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.69400001 0.53839999 0.74720001 0.79159999 0.82999998 0.71359998\n",
      "  0.72320002 0.         0.         0.        ]\n",
      " [0.69400001 0.53839999 0.74720001 0.79159999 0.82999998 0.71280003\n",
      "  0.72280002 0.7252     0.         0.        ]\n",
      " [0.69400001 0.53839999 0.74720001 0.79159999 0.82999998 0.71280003\n",
      "  0.72280002 0.7252     0.6304     0.        ]\n",
      " [0.69400001 0.53839999 0.74720001 0.79159999 0.82999998 0.71280003\n",
      "  0.72280002 0.7252     0.6304     0.81      ]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset(path, labeled=True):\n",
    "    data = torch.load(path)\n",
    "    features = data.get('features', None)\n",
    "    if features is None:\n",
    "        raise ValueError(f\"Features missing in dataset: {path}\")\n",
    "    features = torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "    if labeled:\n",
    "        labels = data.get('labels', None)\n",
    "        if labels is None:\n",
    "            raise ValueError(f\"Labels missing in labeled dataset: {path}\")\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        return features, labels\n",
    "    return features\n",
    "\n",
    "def calculate_prototypes(features, labels, num_classes=10):\n",
    "    prototypes = []\n",
    "    for c in range(num_classes):\n",
    "        class_features = features[labels == c]\n",
    "        if len(class_features) > 0:\n",
    "            prototypes.append(class_features.mean(dim=0))\n",
    "        else:\n",
    "            prototypes.append(torch.zeros(features.shape[1]))\n",
    "    return torch.stack(prototypes)\n",
    "\n",
    "def predict(features, prototypes):\n",
    "    \"\"\"Predict labels based on nearest prototype.\"\"\"\n",
    "    distances = torch.cdist(features, prototypes, p=2)  # L2 distance\n",
    "    return torch.argmin(distances, dim=1)\n",
    "\n",
    "def prototype_contrastive_loss(features, pseudo_labels, prototypes, temperature=0.1):\n",
    "    \"\"\"Compute prototype contrastive loss.\"\"\"\n",
    "    logits = F.cosine_similarity(features.unsqueeze(1), prototypes.unsqueeze(0), dim=2) / temperature\n",
    "    loss = F.cross_entropy(logits, pseudo_labels)\n",
    "    return loss\n",
    "\n",
    "def knowledge_distillation_loss(student_logits, teacher_logits, temperature=2.0):\n",
    "    \"\"\"Compute knowledge distillation loss.\"\"\"\n",
    "    teacher_probs = F.softmax(teacher_logits / temperature, dim=1)\n",
    "    student_probs = F.log_softmax(student_logits / temperature, dim=1)\n",
    "    return F.kl_div(student_probs, teacher_probs, reduction=\"batchmean\") * (temperature ** 2)\n",
    "\n",
    "def train_labeled_dataset(features, labels, num_classes=10):\n",
    "    \"\"\"Train on labeled dataset to compute initial prototypes.\"\"\"\n",
    "    prototypes = calculate_prototypes(features, labels, num_classes)\n",
    "    return torch.nn.Parameter(prototypes)\n",
    "\n",
    "def continual_learning(features, prototypes, prev_model, pseudo_labels, num_classes=10, lr=0.01):\n",
    "    optimizer = torch.optim.SGD([prototypes], lr=lr)\n",
    "    \n",
    "    for _ in range(10):  # Fixed number of iterations\n",
    "        optimizer.zero_grad()\n",
    "        pcl_loss = prototype_contrastive_loss(features, pseudo_labels, prototypes)\n",
    "        if prev_model is not None:\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = F.cosine_similarity(features.unsqueeze(1), prev_model.unsqueeze(0), dim=2)\n",
    "            student_logits = F.cosine_similarity(features.unsqueeze(1), prototypes.unsqueeze(0), dim=2)\n",
    "            kd_loss = knowledge_distillation_loss(student_logits, teacher_logits)\n",
    "            loss = pcl_loss + kd_loss\n",
    "        else:\n",
    "            loss = pcl_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return prototypes\n",
    "\n",
    "def evaluate(features, labels, prototypes):\n",
    "    \"\"\"Evaluate the model and return accuracy.\"\"\"\n",
    "    predictions = predict(features, prototypes)\n",
    "    accuracy = (predictions == labels).float().mean().item()\n",
    "    return accuracy\n",
    "\n",
    "def continual_learning_unlabeled(data_path, initial_prototypes, num_datasets=10):\n",
    "    \"\"\"Continual learning on unlabeled data using pseudo-labels.\"\"\"\n",
    "    prototypes = initial_prototypes\n",
    "    prev_prototypes = None\n",
    "    accuracy_matrix = np.zeros((num_datasets, num_datasets))\n",
    "    \n",
    "    for i in range(1, num_datasets + 1):\n",
    "        train_features = load_dataset(f\"{data_path}/features_dataset_{i}.pth\", labeled=False)\n",
    "        pseudo_labels = predict(train_features, prototypes)\n",
    "        prototypes = continual_learning(train_features, prototypes, prev_prototypes, pseudo_labels)\n",
    "        prev_prototypes = prototypes.detach().clone()\n",
    "        \n",
    "        for j in range(1, i + 1):\n",
    "            eval_features, eval_labels = load_dataset(f\"{data_path}/eval_features_dataset_{j}.pth\")\n",
    "            accuracy_matrix[i - 1, j - 1] = evaluate(eval_features, eval_labels, prototypes)\n",
    "    \n",
    "    return accuracy_matrix\n",
    "\n",
    "def continual_learning_pipeline(data_path_1, data_path_2, eval_data_path, num_datasets=10):\n",
    "    print(\"Training on labeled data from saved_data...\")\n",
    "    train_features, train_labels = load_dataset(f\"{data_path_1}/features_dataset_1.pth\")\n",
    "    initial_prototypes = train_labeled_dataset(train_features, train_labels)\n",
    "\n",
    "    print(\"Continual learning on saved_data2 (unlabeled)...\")\n",
    "    accuracy_matrix = continual_learning_unlabeled(data_path_2, initial_prototypes, num_datasets)\n",
    "    \n",
    "    return accuracy_matrix\n",
    "\n",
    "data_path_1 = \"saved_data\"\n",
    "data_path_2 = \"saved_data2\" \n",
    "eval_data_path = \"saved_data2\" \n",
    "num_datasets = 10\n",
    "\n",
    "accuracy_matrix = continual_learning_pipeline(data_path_1, data_path_2, eval_data_path, num_datasets)\n",
    "print(\"Accuracy Matrix for Saved Data 2:\\n\", accuracy_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
